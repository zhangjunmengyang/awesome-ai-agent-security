#!/usr/bin/env python3
"""
Agent Sentinel Shield ‚Äî Memory Guard Module v0.5.0
====================================================
Protects agent soul/memory files from tampering, drift, and injection.

v1 features (preserved):
- SHA-256 hash verification of critical files
- Git-based diff detection
- Tampering alert with rollback capability
- Canary trap injection and monitoring

v2 features (new):
- Semantic hash via TF-IDF cosine similarity
- Memory source tagging with trust levels (1-5)
- Cognitive file semantic audit (anchor extraction + drift classification)
- Memory injection anomaly detection

Usage:
    python memory_guard.py init <workspace>              # Initialize hash + semantic baseline
    python memory_guard.py check <workspace>             # Check integrity (hash + semantic)
    python memory_guard.py canary <workspace>            # Inject canary traps
    python memory_guard.py audit <workspace>             # Full audit report
    python memory_guard.py tag <workspace> <text> --source <src> --trust <1-5>
    python memory_guard.py semantic-diff <workspace> <filename>
    python memory_guard.py scan-memory <workspace>       # Detect injection anomalies
    python memory_guard.py drift baseline <workspace>    # Build persona baseline from SOUL.md
    python memory_guard.py drift check <workspace> [--json]  # Check persona drift vs baseline
    python memory_guard.py trust init <workspace> [--force]  # Initialize trust topology
    python memory_guard.py trust show <workspace>            # Display trust DAG
    python memory_guard.py trust audit <workspace>           # Audit topology health
    python memory_guard.py trust set <workspace> <from> <to> <weight>  # Set edge weight
"""

import hashlib
import json
import math
import os
import re
import sys
import subprocess
from collections import Counter
from datetime import datetime, timezone
from pathlib import Path

__version__ = "0.5.0"

# =============================================================
# Semantic Embedding (optional ‚Äî graceful fallback to TF-IDF)
# =============================================================

_EMBEDDING_MODEL = None
_EMBEDDING_AVAILABLE = False

def _load_embedding_model():
    """Lazy-load sentence-transformers model. Returns True if available."""
    global _EMBEDDING_MODEL, _EMBEDDING_AVAILABLE
    if _EMBEDDING_MODEL is not None:
        return _EMBEDDING_AVAILABLE
    try:
        from sentence_transformers import SentenceTransformer
        _EMBEDDING_MODEL = SentenceTransformer("all-MiniLM-L6-v2")
        _EMBEDDING_AVAILABLE = True
    except (ImportError, Exception):
        _EMBEDDING_MODEL = False  # Mark as attempted but failed
        _EMBEDDING_AVAILABLE = False
    return _EMBEDDING_AVAILABLE


def _text_to_embedding(text: str) -> list:
    """Convert text to a dense embedding vector (384-dim float list).
    
    Returns empty list if sentence-transformers not available.
    Handles long text by chunking (model max ~256 tokens).
    """
    if not _load_embedding_model():
        return []
    
    # Chunk long text ‚Äî MiniLM handles ~128 word pieces well
    # For long documents, mean-pool chunk embeddings
    import numpy as np
    
    max_chars = 2000  # ~500 tokens, safe for 256-token model
    if len(text) <= max_chars:
        vec = _EMBEDDING_MODEL.encode(text)
        return vec.tolist()
    
    # Split into chunks and mean-pool
    chunks = []
    for i in range(0, len(text), max_chars):
        chunk = text[i:i + max_chars]
        if chunk.strip():
            chunks.append(chunk)
    
    if not chunks:
        return []
    
    embeddings = _EMBEDDING_MODEL.encode(chunks)
    mean_vec = np.mean(embeddings, axis=0)
    # Normalize
    norm = np.linalg.norm(mean_vec)
    if norm > 0:
        mean_vec = mean_vec / norm
    return mean_vec.tolist()


def _cosine_similarity_dense(vec_a: list, vec_b: list) -> float:
    """Cosine similarity between two dense vectors (lists of floats)."""
    if not vec_a or not vec_b or len(vec_a) != len(vec_b):
        return 0.0
    dot = sum(a * b for a, b in zip(vec_a, vec_b))
    mag_a = math.sqrt(sum(a * a for a in vec_a))
    mag_b = math.sqrt(sum(b * b for b in vec_b))
    if mag_a == 0 or mag_b == 0:
        return 0.0
    return dot / (mag_a * mag_b)


# =============================================================
# Configuration
# =============================================================

CRITICAL_FILES = [
    "SOUL.md",
    "AGENTS.md",
    "HEARTBEAT.md",
    "IDENTITY.md",
    "MEMORY.md",
]

MONITORED_FILES = [
    "TOOLS.md",
    "USER.md",
]

HASH_STORE = ".shield/hashes.json"
SEMANTIC_STORE = ".shield/semantic_baselines.json"
CANARY_STORE = ".shield/canaries.json"
TAG_STORE = ".shield/memory_tags.json"
AUDIT_LOG = ".shield/audit.log"

# Semantic similarity thresholds
THRESHOLD_BENIGN = 0.95      # > this = normal update
THRESHOLD_SUSPICIOUS = 0.85  # < this = suspicious drift
THRESHOLD_HOSTILE = 0.70     # < this = likely hostile injection

# Injection indicator patterns
INJECTION_PATTERNS = [
    re.compile(r"ignore\s+previous\s+instructions", re.I),
    re.compile(r"ignore\s+all\s+prior", re.I),
    re.compile(r"you\s+are\s+now\s+", re.I),
    re.compile(r"new\s+system\s+prompt", re.I),
    re.compile(r"override\s+(your|all)\s+(rules|instructions)", re.I),
    re.compile(r"disregard\s+(your|all|the)", re.I),
    re.compile(r"forget\s+(everything|all|your)", re.I),
    re.compile(r"replace\s+your\s+(identity|personality|soul|role)", re.I),
    re.compile(r"act\s+as\s+if\s+you", re.I),
    re.compile(r"pretend\s+(you|to\s+be)", re.I),
    re.compile(r"from\s+now\s+on\s+you\s+are", re.I),
    re.compile(r"ÂøΩÁï•.{0,10}(Êåá‰ª§|ËßÑÂàô|ËÆæÂÆö)", re.I),
    re.compile(r"‰Ω†Áé∞Âú®ÊòØ", re.I),
    re.compile(r"Ë¶ÜÁõñ.{0,10}(Ë∫´‰ªΩ|‰∫∫Ê†º|ÁÅµÈ≠Ç)", re.I),
    re.compile(r"ÊõøÊç¢.{0,10}(Ë∫´‰ªΩ|‰∫∫Ê†º|ÁÅµÈ≠Ç|ËßíËâ≤)", re.I),
]

# Trust level defaults by source type
SOURCE_TRUST_DEFAULTS = {
    "owner_direct": 5,
    "agent_self": 4,
    "agent_peer": 3,
    "external_tool": 2,
    "web_scrape": 1,
    "unknown": 1,
}

# =============================================================
# Utility: SHA-256 hashing
# =============================================================

def sha256_file(filepath: str) -> str:
    """Compute SHA-256 hash of a file."""
    h = hashlib.sha256()
    with open(filepath, "rb") as f:
        for chunk in iter(lambda: f.read(8192), b""):
            h.update(chunk)
    return h.hexdigest()


# =============================================================
# Utility: Lightweight TF-IDF (no sklearn dependency)
# =============================================================

def _tokenize(text: str) -> list:
    """Simple tokenizer: lowercase, split on non-alphanumeric, filter short."""
    tokens = re.findall(r"[a-z\u4e00-\u9fff]+[a-z0-9\u4e00-\u9fff]*", text.lower())
    return [t for t in tokens if len(t) > 1]


def _compute_tf(tokens: list) -> dict:
    """Term frequency: count / total."""
    counts = Counter(tokens)
    total = len(tokens) if tokens else 1
    return {t: c / total for t, c in counts.items()}


def _cosine_similarity(vec_a: dict, vec_b: dict) -> float:
    """Cosine similarity between two sparse vectors (dicts)."""
    if not vec_a or not vec_b:
        return 0.0
    common_keys = set(vec_a.keys()) & set(vec_b.keys())
    dot = sum(vec_a[k] * vec_b[k] for k in common_keys)
    mag_a = math.sqrt(sum(v * v for v in vec_a.values()))
    mag_b = math.sqrt(sum(v * v for v in vec_b.values()))
    if mag_a == 0 or mag_b == 0:
        return 0.0
    return dot / (mag_a * mag_b)


def _text_to_tfidf_vector(text: str) -> dict:
    """Convert text to a TF vector (IDF skipped for single-doc comparison; TF alone is sufficient for cosine drift detection)."""
    tokens = _tokenize(text)
    return _compute_tf(tokens)


def _classify_drift(similarity: float) -> str:
    """Classify semantic change based on cosine similarity."""
    if similarity >= THRESHOLD_BENIGN:
        return "benign_update"
    elif similarity >= THRESHOLD_SUSPICIOUS:
        return "minor_drift"
    elif similarity >= THRESHOLD_HOSTILE:
        return "suspicious_drift"
    else:
        return "hostile_injection"


# =============================================================
# Utility: Anchor extraction for cognitive files
# =============================================================

def _extract_anchors(text: str) -> list:
    """Extract semantic anchors from markdown: headings, bold terms, list items."""
    anchors = []
    for line in text.splitlines():
        line = line.strip()
        # Headings
        if line.startswith("#"):
            anchors.append(("heading", re.sub(r"^#+\s*", "", line)))
        # Bold terms
        for m in re.finditer(r"\*\*(.+?)\*\*", line):
            anchors.append(("bold", m.group(1)))
        # List items (top-level content)
        if re.match(r"^[-*]\s+", line):
            content = re.sub(r"^[-*]\s+", "", line)
            if len(content) > 10:
                anchors.append(("list", content[:120]))
    return anchors


def _detect_injections(text: str) -> list:
    """Scan text for injection indicator patterns. Returns list of (pattern_desc, match)."""
    found = []
    for pat in INJECTION_PATTERNS:
        for m in pat.finditer(text):
            found.append((pat.pattern, m.group()))
    return found


# =============================================================
# Storage helpers
# =============================================================

def _ensure_shield_dir(workspace: str):
    os.makedirs(os.path.join(workspace, ".shield"), exist_ok=True)


def _load_json(workspace: str, store: str) -> dict:
    path = os.path.join(workspace, store)
    if os.path.exists(path):
        with open(path) as f:
            return json.load(f)
    return {}


def _save_json(workspace: str, store: str, data):
    _ensure_shield_dir(workspace)
    path = os.path.join(workspace, store)
    with open(path, "w") as f:
        json.dump(data, f, indent=2, ensure_ascii=False)


def log_event(workspace: str, event: str, severity: str = "INFO"):
    """Append to audit log."""
    _ensure_shield_dir(workspace)
    log_path = os.path.join(workspace, AUDIT_LOG)
    ts = datetime.now(timezone.utc).isoformat()
    with open(log_path, "a") as f:
        f.write(f"[{ts}] [{severity}] {event}\n")


# =============================================================
# Command: init ‚Äî Initialize hash + semantic baseline
# =============================================================

def init_baseline(workspace: str):
    """Initialize hash and semantic baselines for all critical files."""
    hashes = {}
    semantics = {}

    for fname in CRITICAL_FILES + MONITORED_FILES:
        fpath = os.path.join(workspace, fname)
        if os.path.exists(fpath):
            h = sha256_file(fpath)
            with open(fpath, "r", errors="replace") as f:
                content = f.read()

            tf_vec = _text_to_tfidf_vector(content)
            anchors = _extract_anchors(content)

            hashes[fname] = {
                "hash": h,
                "size": os.path.getsize(fpath),
                "mtime": os.path.getmtime(fpath),
                "initialized_at": datetime.now(timezone.utc).isoformat(),
            }
            semantics[fname] = {
                "tf_vector": tf_vec,
                "anchor_count": len(anchors),
                "anchors_digest": [f"{a[0]}:{a[1][:60]}" for a in anchors[:50]],
                "content_length": len(content),
                "initialized_at": datetime.now(timezone.utc).isoformat(),
            }
            print(f"  ‚úÖ {fname}: hash={h[:16]}... anchors={len(anchors)} tokens={len(tf_vec)}")
        else:
            print(f"  ‚ö†Ô∏è  {fname}: not found (skipped)")

    _save_json(workspace, HASH_STORE, hashes)
    _save_json(workspace, SEMANTIC_STORE, semantics)
    log_event(workspace, f"Baseline initialized with {len(hashes)} files (hash + semantic)")
    print(f"\nüõ°Ô∏è  Baseline saved. Protected files: {len(hashes)}")


# =============================================================
# Command: check ‚Äî Check integrity (hash + semantic)
# =============================================================

def check_integrity(workspace: str) -> list:
    """Check all critical files against hash and semantic baselines."""
    stored_hashes = _load_json(workspace, HASH_STORE)
    stored_semantics = _load_json(workspace, SEMANTIC_STORE)

    if not stored_hashes:
        print("‚ùå No baseline found. Run 'init' first.")
        return [{"file": "BASELINE", "type": "missing"}]

    violations = []

    for fname in CRITICAL_FILES:
        fpath = os.path.join(workspace, fname)
        stored_h = stored_hashes.get(fname)

        if not os.path.exists(fpath):
            if stored_h:
                v = {"file": fname, "type": "deleted", "severity": "CRITICAL"}
                violations.append(v)
                log_event(workspace, f"CRITICAL: {fname} DELETED", "CRITICAL")
                print(f"  üî¥ {fname}: DELETED!")
            continue

        if not stored_h:
            print(f"  ‚ö†Ô∏è  {fname}: exists but not in baseline")
            continue

        current_hash = sha256_file(fpath)

        if current_hash == stored_h["hash"]:
            print(f"  ‚úÖ {fname}: intact")
            continue

        # Hash changed ‚Äî do semantic analysis
        size_delta = os.path.getsize(fpath) - stored_h.get("size", 0)

        with open(fpath, "r", errors="replace") as f:
            current_content = f.read()

        # Semantic similarity
        sem_baseline = stored_semantics.get(fname, {})
        baseline_vec = sem_baseline.get("tf_vector", {})
        current_vec = _text_to_tfidf_vector(current_content)
        similarity = _cosine_similarity(baseline_vec, current_vec)
        drift_class = _classify_drift(similarity)

        # Injection scan
        injections = _detect_injections(current_content)

        # Override classification if injections found
        if injections and drift_class in ("benign_update", "minor_drift"):
            drift_class = "suspicious_drift"

        # Determine severity
        if drift_class == "hostile_injection":
            severity = "CRITICAL"
        elif drift_class == "suspicious_drift":
            severity = "HIGH"
        elif drift_class == "minor_drift":
            severity = "MEDIUM"
        else:
            severity = "LOW"

        v = {
            "file": fname,
            "type": "modified",
            "severity": severity,
            "drift_class": drift_class,
            "semantic_similarity": round(similarity, 4),
            "old_hash": stored_h["hash"][:16],
            "new_hash": current_hash[:16],
            "size_delta": size_delta,
            "injection_indicators": len(injections),
        }
        violations.append(v)

        # Severity-appropriate emoji
        emoji = {"CRITICAL": "üî¥", "HIGH": "üü†", "MEDIUM": "üü°", "LOW": "üü¢"}.get(severity, "‚ö™")
        log_event(workspace, f"{severity}: {fname} modified ‚Äî {drift_class} (sim={similarity:.3f}, injections={len(injections)})", severity)
        print(f"  {emoji} {fname}: {drift_class} (similarity={similarity:.3f}, Œîsize={size_delta:+d})")
        if injections:
            print(f"     ‚ö†Ô∏è  {len(injections)} injection indicator(s) detected!")
            for pat, match in injections[:3]:
                print(f"        ‚Üí \"{match}\"")

        _show_git_diff(workspace, fname)

    # Monitored files (non-critical, hash only)
    for fname in MONITORED_FILES:
        fpath = os.path.join(workspace, fname)
        stored_h = stored_hashes.get(fname)
        if stored_h and os.path.exists(fpath):
            if sha256_file(fpath) != stored_h["hash"]:
                print(f"  üü° {fname}: modified (monitored)")
                log_event(workspace, f"WARNING: {fname} modified", "WARNING")

    if not violations:
        log_event(workspace, "Integrity check passed ‚Äî all files intact")
        print("\nüõ°Ô∏è  All critical files intact.")
    else:
        crit = sum(1 for v in violations if v.get("severity") == "CRITICAL")
        high = sum(1 for v in violations if v.get("severity") == "HIGH")
        print(f"\nüö® {len(violations)} violation(s): {crit} CRITICAL, {high} HIGH")

    return violations


def _show_git_diff(workspace: str, fname: str):
    """Try to show git diff for a modified file."""
    try:
        result = subprocess.run(
            ["git", "diff", "--stat", fname],
            cwd=workspace, capture_output=True, text=True, timeout=5,
        )
        if result.stdout.strip():
            print(f"     git: {result.stdout.strip()}")
    except (subprocess.TimeoutExpired, FileNotFoundError):
        pass


# =============================================================
# Command: canary ‚Äî Inject canary traps
# =============================================================

def inject_canaries(workspace: str):
    """Inject canary traps into memory files."""
    import secrets

    canaries = {}
    memory_path = os.path.join(workspace, "MEMORY.md")

    if not os.path.exists(memory_path):
        print("‚ùå MEMORY.md not found.")
        return

    for i in range(3):
        token = f"canary-{secrets.token_hex(4)}"
        canaries[token] = {
            "injected_at": datetime.now(timezone.utc).isoformat(),
            "location": "MEMORY.md",
            "type": ["api_key", "internal_url", "project_name"][i],
        }

    _save_json(workspace, CANARY_STORE, canaries)
    log_event(workspace, f"Injected {len(canaries)} canary traps")
    print(f"üê§ Injected {len(canaries)} canary traps.")
    for token, info in canaries.items():
        print(f"   [{info['type']}] {token}")


def check_canary_leak(text: str, workspace: str) -> list:
    """Check if any canary tokens appear in given text."""
    canaries = _load_json(workspace, CANARY_STORE)
    leaked = []
    for token in canaries:
        if token in text:
            leaked.append(token)
            log_event(workspace, f"CANARY LEAKED: {token}!", "CRITICAL")
    return leaked


# =============================================================
# Command: tag ‚Äî Memory source tagging
# =============================================================

def tag_memory(workspace: str, text: str, source: str, trust: int = None, session_id: str = None):
    """Tag a memory entry with source and trust level."""
    if source not in SOURCE_TRUST_DEFAULTS:
        print(f"‚ùå Unknown source type: {source}")
        print(f"   Valid: {', '.join(SOURCE_TRUST_DEFAULTS.keys())}")
        return

    if trust is None:
        trust = SOURCE_TRUST_DEFAULTS[source]
    trust = max(1, min(5, trust))

    tags = _load_json(workspace, TAG_STORE)
    if not isinstance(tags, list):
        tags = []

    entry = {
        "content_hash": hashlib.sha256(text.encode()).hexdigest()[:16],
        "content_preview": text[:100].replace("\n", " "),
        "source": source,
        "trust_level": trust,
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "session_id": session_id or "unknown",
    }
    tags.append(entry)
    _save_json(workspace, TAG_STORE, tags)

    trust_emoji = ["", "üî¥", "üü†", "üü°", "üîµ", "üü¢"][trust]
    print(f"  {trust_emoji} Tagged [{source}] trust={trust}: {text[:80]}...")

    # Warn if low-trust content
    if trust <= 2:
        print(f"  ‚ö†Ô∏è  Low-trust content (level {trust}). Must NOT be written to cognitive files.")
        log_event(workspace, f"Low-trust memory tagged: source={source}, trust={trust}", "WARNING")


# =============================================================
# Command: semantic-diff ‚Äî Cognitive file semantic audit
# =============================================================

def semantic_diff(workspace: str, filename: str):
    """Compare a cognitive file's current state against its semantic baseline."""
    semantics = _load_json(workspace, SEMANTIC_STORE)
    baseline = semantics.get(filename)

    if not baseline:
        print(f"‚ùå No semantic baseline for {filename}. Run 'init' first.")
        return

    fpath = os.path.join(workspace, filename)
    if not os.path.exists(fpath):
        print(f"‚ùå {filename} not found.")
        return

    with open(fpath, "r", errors="replace") as f:
        current_content = f.read()

    # Semantic similarity
    baseline_vec = baseline.get("tf_vector", {})
    current_vec = _text_to_tfidf_vector(current_content)
    similarity = _cosine_similarity(baseline_vec, current_vec)
    drift_class = _classify_drift(similarity)

    # Anchor comparison
    current_anchors = _extract_anchors(current_content)
    baseline_anchor_set = set(baseline.get("anchors_digest", []))
    current_anchor_set = set(f"{a[0]}:{a[1][:60]}" for a in current_anchors[:50])

    new_anchors = current_anchor_set - baseline_anchor_set
    removed_anchors = baseline_anchor_set - current_anchor_set

    # Injection scan
    injections = _detect_injections(current_content)

    # Report
    emoji = {"benign_update": "üü¢", "minor_drift": "üü°", "suspicious_drift": "üü†", "hostile_injection": "üî¥"}.get(drift_class, "‚ö™")

    print(f"\n{'=' * 60}")
    print(f"üõ°Ô∏è  Semantic Diff: {filename}")
    print(f"{'=' * 60}")
    print(f"  Classification: {emoji} {drift_class}")
    print(f"  Cosine similarity: {similarity:.4f}")
    print(f"  Size: {baseline.get('content_length', '?')} ‚Üí {len(current_content)} chars")
    print(f"  Anchors: {baseline.get('anchor_count', '?')} ‚Üí {len(current_anchors)}")

    if new_anchors:
        print(f"\n  üì• New anchors ({len(new_anchors)}):")
        for a in sorted(new_anchors)[:10]:
            print(f"     + {a}")

    if removed_anchors:
        print(f"\n  üì§ Removed anchors ({len(removed_anchors)}):")
        for a in sorted(removed_anchors)[:10]:
            print(f"     - {a}")

    if injections:
        print(f"\n  üö® Injection indicators ({len(injections)}):")
        for pat, match in injections[:5]:
            print(f"     ‚Üí \"{match}\"")

    if not new_anchors and not removed_anchors and not injections:
        print(f"\n  No significant structural changes.")

    print(f"{'=' * 60}")
    log_event(workspace, f"semantic-diff {filename}: {drift_class} (sim={similarity:.3f})", "INFO")


# =============================================================
# Command: scan-memory ‚Äî Memory injection anomaly detection
# =============================================================

def scan_memory(workspace: str):
    """Scan memory/*.md files for injection anomalies."""
    memory_dir = os.path.join(workspace, "memory")
    if not os.path.isdir(memory_dir):
        print(f"‚ùå memory/ directory not found in {workspace}")
        return

    anomalies = []
    files = sorted(Path(memory_dir).glob("*.md"))

    if not files:
        print("  No memory files found.")
        return

    print(f"üîç Scanning {len(files)} memory files...\n")

    for fpath in files:
        fname = fpath.name
        stat = fpath.stat()
        size_kb = stat.st_size / 1024

        with open(fpath, "r", errors="replace") as f:
            content = f.read()

        issues = []

        # Check 1: Unusually large file (>50KB)
        if size_kb > 50:
            issues.append(f"Large file: {size_kb:.1f}KB (threshold: 50KB)")

        # Check 2: Injection patterns
        injections = _detect_injections(content)
        if injections:
            issues.append(f"{len(injections)} injection indicator(s)")

        # Check 3: Suspicious patterns ‚Äî base64 blocks, URLs to known exfil endpoints
        base64_blocks = re.findall(r"[A-Za-z0-9+/=]{100,}", content)
        if base64_blocks:
            issues.append(f"{len(base64_blocks)} base64 block(s) (possible obfuscation)")

        # Check 4: Non-markdown executable content
        code_blocks = re.findall(r"```(?:bash|sh|python|js)\n(.+?)```", content, re.S)
        executable_lines = 0
        for block in code_blocks:
            for line in block.splitlines():
                line = line.strip()
                if any(p in line for p in ["curl ", "wget ", "eval(", "exec(", "rm -rf", "chmod 777"]):
                    executable_lines += 1
        if executable_lines:
            issues.append(f"{executable_lines} dangerous executable line(s) in code blocks")

        if issues:
            anomalies.append({"file": fname, "issues": issues})
            print(f"  üü† {fname}:")
            for issue in issues:
                print(f"     ‚Üí {issue}")
        else:
            # Only print clean files in verbose mode
            pass

    if not anomalies:
        print("  ‚úÖ No anomalies detected in memory files.")
    else:
        print(f"\nüö® {len(anomalies)} file(s) with anomalies.")

    log_event(workspace, f"scan-memory: {len(files)} files scanned, {len(anomalies)} anomalies", "INFO")
    return anomalies


# =============================================================
# Command: audit ‚Äî Full security audit (v2 enhanced)
# =============================================================

def full_audit(workspace: str):
    """Run a complete security audit (v2)."""
    print("=" * 60)
    print(f"üõ°Ô∏è  Agent Sentinel Shield v{__version__} ‚Äî Full Audit")
    print(f"   Workspace: {workspace}")
    print(f"   Time: {datetime.now(timezone.utc).isoformat()}")
    print("=" * 60)

    # 1. Hash + semantic integrity
    print("\nüìã 1. File Integrity Check (Hash + Semantic)")
    print("-" * 40)
    violations = check_integrity(workspace)

    # 2. Memory injection scan
    print("\nüìã 2. Memory Injection Scan")
    print("-" * 40)
    anomalies = scan_memory(workspace)

    # 3. Memory tag summary
    print("\nüìã 3. Memory Source Tags")
    print("-" * 40)
    tags = _load_json(workspace, TAG_STORE)
    if isinstance(tags, list) and tags:
        by_source = Counter(t.get("source", "unknown") for t in tags)
        by_trust = Counter(t.get("trust_level", 0) for t in tags)
        print(f"  Total tagged entries: {len(tags)}")
        print(f"  By source: {dict(by_source)}")
        print(f"  By trust:  {dict(sorted(by_trust.items()))}")
        low_trust = sum(1 for t in tags if t.get("trust_level", 0) <= 2)
        if low_trust:
            print(f"  ‚ö†Ô∏è  {low_trust} low-trust entries (level ‚â§ 2)")
    else:
        print("  No tagged entries yet. Use 'tag' to start tracking memory sources.")

    # 4. Canary status
    print("\nüìã 4. Canary Trap Status")
    print("-" * 40)
    canaries = _load_json(workspace, CANARY_STORE)
    if canaries:
        print(f"  Active canaries: {len(canaries)}")
        for token, info in canaries.items():
            print(f"    [{info['type']}] {token[:12]}... (since {info['injected_at'][:10]})")
    else:
        print("  ‚ö†Ô∏è  No canaries deployed. Run 'canary' to set up.")

    # 5. Recent audit events
    print("\nüìã 5. Recent Audit Events")
    print("-" * 40)
    log_path = os.path.join(workspace, AUDIT_LOG)
    if os.path.exists(log_path):
        with open(log_path) as f:
            lines = f.readlines()
        for line in lines[-10:]:
            print(f"  {line.rstrip()}")
    else:
        print("  No audit events yet.")

    # 6. Summary
    print("\n" + "=" * 60)
    n_violations = len(violations) if violations else 0
    n_anomalies = len(anomalies) if anomalies else 0
    if n_violations or n_anomalies:
        print(f"üö® AUDIT: {n_violations} integrity violation(s), {n_anomalies} memory anomalie(s)")
    else:
        print("‚úÖ AUDIT PASSED ‚Äî No violations or anomalies detected")
    print("=" * 60)


# =============================================================
# Drift detection: constants and storage paths
# =============================================================

DRIFT_BASELINE_STORE = ".shield/drift_baseline.json"
DRIFT_HISTORY_STORE = ".shield/drift_history.json"
TRUST_TOPOLOGY_STORE = ".shield/trust_topology.json"

# 5-level drift thresholds ‚Äî TF-IDF (sparse, vocabulary-dependent)
DRIFT_THRESHOLDS_TFIDF = {
    "green": 0.85,     # similarity >= 0.85 = normal
    "yellow": 0.75,    # 0.75 <= sim < 0.85 = mild drift
    "orange": 0.60,    # 0.60 <= sim < 0.75 = medium drift
    "red": 0.45,       # 0.45 <= sim < 0.60 = severe drift
    "black": 0.0,      # sim < 0.45 = persona lost
}

# 5-level drift thresholds ‚Äî Semantic Embedding (dense, meaning-aware)
# Calibrated from real-world data (2026-02-21):
#   Full SOUL.md vs active session: ~0.56 (should be GREEN)
#   Dao section vs active session:  ~0.55 (should be GREEN)
#   SOUL.md vs Identity text:       ~0.58 (should be GREEN)
#   SOUL.md vs Work output:         ~0.45 (should be YELLOW ‚Äî normal divergence)
#   SOUL.md vs Random text:         ~0.15 (should be RED/BLACK)
DRIFT_THRESHOLDS_SEMANTIC = {
    "green": 0.45,     # similarity >= 0.45 = normal (cross-domain persona intact)
    "yellow": 0.35,    # 0.35 <= sim < 0.45 = mild drift
    "orange": 0.25,    # 0.25 <= sim < 0.35 = medium drift
    "red": 0.15,       # 0.15 <= sim < 0.25 = severe drift
    "black": 0.0,      # sim < 0.15 = persona lost
}

# Separate thresholds for short anchor texts (single quotes, < 50 chars)
# Short text embeddings naturally have lower similarity to long documents
DRIFT_THRESHOLDS_ANCHOR = {
    "green": 0.30,     # Short anchor >= 0.30 = intact
    "yellow": 0.22,    # 0.22 <= sim < 0.30 = mild
    "orange": 0.15,    # 0.15 <= sim < 0.22 = medium
    "red": 0.08,       # 0.08 <= sim < 0.15 = severe
    "black": 0.0,      # sim < 0.08 = lost
}

# Active thresholds (selected at runtime based on embedding availability)
DRIFT_THRESHOLDS = DRIFT_THRESHOLDS_TFIDF  # Default, overridden in drift_check()

DRIFT_LEVEL_DISPLAY = {
    "green":  ("üü¢", "GREEN",  "Normal ‚Äî persona intact"),
    "yellow": ("üü°", "YELLOW", "Mild drift detected"),
    "orange": ("üü†", "ORANGE", "Medium drift detected"),
    "red":    ("üî¥", "RED",    "Severe drift detected"),
    "black":  ("‚ö´", "BLACK",  "Persona lost"),
}

DRIFT_RECOMMENDATIONS = {
    "green":  "No action needed.",
    "yellow": "Monitor closely; consider persona reinforcement in next session.",
    "orange": "Inject periodic persona reinforcement.",
    "red":    "Immediate persona recalibration required. Re-read SOUL.md.",
    "black":  "CRITICAL: Full persona reset needed. Halt operations and rebuild from SOUL.md.",
}


def _classify_drift_level(similarity: float, thresholds: dict = None) -> str:
    """Classify drift level based on 5-tier threshold system.

    Returns one of: green, yellow, orange, red, black.
    Uses DRIFT_THRESHOLDS_SEMANTIC or DRIFT_THRESHOLDS_TFIDF based on caller.
    """
    t = thresholds or DRIFT_THRESHOLDS_TFIDF
    if similarity >= t["green"]:
        return "green"
    elif similarity >= t["yellow"]:
        return "yellow"
    elif similarity >= t["orange"]:
        return "orange"
    elif similarity >= t["red"]:
        return "red"
    else:
        return "black"


def _extract_dao_section(text: str) -> str:
    """Extract the '## ÈÅì' section from SOUL.md (text between ## ÈÅì and next ##)."""
    match = re.search(r"^## ÈÅì\s*\n(.*?)(?=^## |\Z)", text, re.M | re.S)
    if match:
        return match.group(1).strip()
    return ""


def _extract_quote_anchors(text: str) -> list:
    """Extract all blockquote lines (> ...) as value anchors.

    Returns list of cleaned quote strings (without '> ' prefix).
    """
    anchors = []
    for line in text.splitlines():
        line = line.strip()
        if line.startswith("> ") and not line.startswith("> ‚Äî"):
            # Skip attribution lines (> ‚Äî Author)
            cleaned = line[2:].strip()
            if cleaned and len(cleaned) > 1:
                anchors.append(cleaned)
    return anchors


def _extract_identity_keywords(tf_vector: dict, top_n: int = 20) -> list:
    """Extract top-N high-frequency identity keywords from a TF vector."""
    sorted_terms = sorted(tf_vector.items(), key=lambda x: x[1], reverse=True)
    return [term for term, _ in sorted_terms[:top_n]]


# =============================================================
# Command: drift baseline ‚Äî Build persona baseline from SOUL.md
# =============================================================

def drift_baseline(workspace: str):
    """Build persona drift baseline from SOUL.md.

    Extracts the Dao section, value anchors (blockquotes), computes
    TF-IDF vectors for the full text and each anchor, and stores
    the result to .shield/drift_baseline.json.
    """
    soul_path = os.path.join(workspace, "SOUL.md")
    if not os.path.exists(soul_path):
        print("‚ùå SOUL.md not found in workspace.")
        sys.exit(1)

    with open(soul_path, "r", errors="replace") as f:
        soul_text = f.read()

    source_hash = hashlib.sha256(soul_text.encode("utf-8")).hexdigest()

    # Extract Dao section
    dao_section = _extract_dao_section(soul_text)
    if not dao_section:
        print("‚ö†Ô∏è  Could not extract '## ÈÅì' section. Using full text as fallback.")

    # Extract value anchors (blockquotes)
    anchors_text = _extract_quote_anchors(soul_text)
    if not anchors_text:
        print("‚ö†Ô∏è  No blockquote anchors found in SOUL.md.")

    # Compute TF-IDF vectors (always available ‚Äî pure stdlib)
    full_vector = _text_to_tfidf_vector(soul_text)
    anchor_entries = []
    for anchor in anchors_text:
        vec = _text_to_tfidf_vector(anchor)
        entry = {"text": anchor, "vector": vec}
        anchor_entries.append(entry)

    # Extract identity keywords
    identity_keywords = _extract_identity_keywords(full_vector)

    # Compute semantic embeddings (if sentence-transformers available)
    has_semantic = _load_embedding_model()
    full_embedding = []
    if has_semantic:
        print("   Computing semantic embeddings (all-MiniLM-L6-v2)...")
        full_embedding = _text_to_embedding(soul_text)
        # Also embed each anchor
        for entry in anchor_entries:
            entry["embedding"] = _text_to_embedding(entry["text"])
        # Embed the Dao section specifically (strongest persona signal)
        dao_embedding = _text_to_embedding(dao_section) if dao_section else []
    else:
        print("   ‚ö†Ô∏è  sentence-transformers not available ‚Äî TF-IDF only baseline")
        dao_embedding = []

    baseline = {
        "version": "0.3.0",
        "created_at": datetime.now(timezone.utc).isoformat(),
        "source_file": "SOUL.md",
        "source_hash": source_hash,
        "full_text_vector": full_vector,
        "full_embedding": full_embedding,
        "dao_embedding": dao_embedding,
        "has_semantic": has_semantic,
        "anchors": anchor_entries,
        "identity_keywords": identity_keywords,
    }

    _save_json(workspace, DRIFT_BASELINE_STORE, baseline)
    log_event(workspace, f"Drift baseline created from SOUL.md (hash={source_hash[:16]}, anchors={len(anchor_entries)}, keywords={len(identity_keywords)})")

    print(f"üõ°Ô∏è  Drift baseline created.")
    print(f"   Source: SOUL.md (sha256: {source_hash[:16]}...)")
    print(f"   Dao section: {len(dao_section)} chars")
    print(f"   Anchors: {len(anchor_entries)}")
    for i, a in enumerate(anchor_entries):
        has_emb = "‚úì" if a.get("embedding") else "‚úó"
        print(f"     [{i+1}] \"{a['text'][:60]}\" [emb:{has_emb}]")
    print(f"   Identity keywords: {', '.join(identity_keywords[:10])}...")
    print(f"   TF-IDF vectors: {len(full_vector)} terms")
    if has_semantic:
        print(f"   Semantic embedding: {len(full_embedding)}-dim (all-MiniLM-L6-v2)")
        print(f"   Dao embedding: {len(dao_embedding)}-dim")
    else:
        print(f"   Semantic embedding: NOT AVAILABLE (install sentence-transformers)")
    print(f"   Saved to: .shield/drift_baseline.json")


# =============================================================
# Command: drift check ‚Äî Check persona drift vs baseline
# =============================================================

def _extract_session_assistant_text(workspace: str, max_chars: int = 50000) -> tuple:
    """Extract assistant message text from the most recent session JSONL.

    Searches for OpenClaw session JSONL files under ~/.openclaw/agents/main/sessions/
    and extracts text content from assistant messages.

    Returns (source_label, text) or (None, None) if not found.
    """
    session_dirs = [
        os.path.expanduser("~/.openclaw/agents/main/sessions"),
        os.path.join(workspace, "..", ".openclaw", "agents", "main", "sessions"),
    ]

    jsonl_files = []
    for d in session_dirs:
        if os.path.isdir(d):
            for f in os.listdir(d):
                if f.endswith(".jsonl"):
                    full = os.path.join(d, f)
                    jsonl_files.append((os.path.getmtime(full), full))

    if not jsonl_files:
        return None, None

    # Pick the most recently modified
    jsonl_files.sort(reverse=True)
    latest_jsonl = jsonl_files[0][1]
    label = os.path.basename(latest_jsonl)

    texts = []
    total = 0
    try:
        with open(latest_jsonl, "r", errors="replace") as f:
            for line in f:
                if '"role":"assistant"' not in line:
                    continue
                try:
                    d = json.loads(line)
                    msg = d.get("message", d)  # message wrapper or direct
                    content = msg.get("content", "")
                    if isinstance(content, list):
                        for item in content:
                            if isinstance(item, dict) and item.get("type") == "text":
                                t = item.get("text", "")
                                texts.append(t)
                                total += len(t)
                    elif isinstance(content, str) and content:
                        texts.append(content)
                        total += len(content)
                    if total >= max_chars:
                        break
                except (json.JSONDecodeError, KeyError):
                    continue
    except OSError:
        return None, None

    if not texts:
        return None, None

    return f"session/{label}", "\n".join(texts)


def drift_check(workspace: str, output_json: bool = False, source: str = "auto"):
    """Check persona drift by comparing recent output against baseline.

    source options:
      - "auto": try session JSONL first, fall back to memory/*.md
      - "session": extract from latest session JSONL (assistant messages)
      - "memory": use latest memory/YYYY-MM-DD.md

    Loads the drift baseline, extracts target text, computes TF-IDF
    similarity, classifies drift level, and outputs the assessment.
    Results are appended to .shield/drift_history.json.
    """
    # Load baseline
    baseline = _load_json(workspace, DRIFT_BASELINE_STORE)
    if not baseline or "full_text_vector" not in baseline:
        print("‚ùå No drift baseline found. Run 'drift baseline' first.")
        sys.exit(1)

    target_text = None
    target_label = None

    # Try session source
    if source in ("auto", "session"):
        sess_label, sess_text = _extract_session_assistant_text(workspace)
        if sess_text:
            target_text = sess_text
            target_label = sess_label

    # Fall back to memory
    if target_text is None and source in ("auto", "memory"):
        memory_dir = os.path.join(workspace, "memory")
        if os.path.isdir(memory_dir):
            memory_files = sorted(
                [f for f in os.listdir(memory_dir)
                 if re.match(r"\d{4}-\d{2}-\d{2}\.md$", f)]
            )
            if memory_files:
                latest_file = memory_files[-1]
                target_path = os.path.join(memory_dir, latest_file)
                with open(target_path, "r", errors="replace") as f:
                    target_text = f.read()
                target_label = f"memory/{latest_file}"

    if target_text is None:
        print("‚ùå No target text found (no session JSONL or memory/*.md).")
        sys.exit(1)

    # Determine which similarity method to use
    use_semantic = (
        baseline.get("has_semantic", False)
        and baseline.get("full_embedding")
        and _load_embedding_model()
    )
    
    if use_semantic:
        method = "semantic"
        thresholds = DRIFT_THRESHOLDS_SEMANTIC
        # Compute target embedding
        target_embedding = _text_to_embedding(target_text)
        
        # Overall similarity (semantic)
        baseline_embedding = baseline["full_embedding"]
        overall_sim = _cosine_similarity_dense(baseline_embedding, target_embedding)
        overall_level = _classify_drift_level(overall_sim, thresholds)
        
        # Also compute Dao section similarity (strongest persona signal)
        dao_sim = None
        if baseline.get("dao_embedding"):
            dao_sim = _cosine_similarity_dense(baseline["dao_embedding"], target_embedding)
        
        # Per-anchor similarity (semantic)
        # Use anchor-specific thresholds for short text anchors
        anchor_results = []
        for anchor in baseline.get("anchors", []):
            anchor_emb = anchor.get("embedding", [])
            if anchor_emb:
                sim = _cosine_similarity_dense(anchor_emb, target_embedding)
            else:
                # Fallback to TF-IDF for anchors without embeddings
                anchor_vec = anchor.get("vector", {})
                target_vector = _text_to_tfidf_vector(target_text)
                sim = _cosine_similarity(anchor_vec, target_vector)
            # Short anchors (< 50 chars) get relaxed thresholds
            anchor_thresh = DRIFT_THRESHOLDS_ANCHOR if len(anchor["text"]) < 50 else thresholds
            level = _classify_drift_level(sim, anchor_thresh)
            anchor_results.append({
                "text": anchor["text"],
                "similarity": round(sim, 4),
                "level": level,
            })
    else:
        method = "tfidf"
        thresholds = DRIFT_THRESHOLDS_TFIDF
        # Compute target vector (TF-IDF)
        target_vector = _text_to_tfidf_vector(target_text)
        
        # Overall similarity
        baseline_vector = baseline["full_text_vector"]
        overall_sim = _cosine_similarity(baseline_vector, target_vector)
        overall_level = _classify_drift_level(overall_sim, thresholds)
        dao_sim = None
        
        # Per-anchor similarity
        anchor_results = []
        for anchor in baseline.get("anchors", []):
            anchor_vec = anchor.get("vector", {})
            sim = _cosine_similarity(anchor_vec, target_vector)
            level = _classify_drift_level(sim, thresholds)
            anchor_results.append({
                "text": anchor["text"],
                "similarity": round(sim, 4),
                "level": level,
            })

    # Build result record
    result = {
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "baseline_hash": baseline.get("source_hash", "unknown"),
        "target_file": target_label,
        "method": method,
        "overall_similarity": round(overall_sim, 4),
        "drift_level": overall_level,
        "anchor_results": anchor_results,
        "recommendation": DRIFT_RECOMMENDATIONS[overall_level],
    }
    if dao_sim is not None:
        result["dao_similarity"] = round(dao_sim, 4)

    # Append to history
    history = _load_json(workspace, DRIFT_HISTORY_STORE)
    if not isinstance(history, list):
        history = []
    history.append(result)
    _save_json(workspace, DRIFT_HISTORY_STORE, history)

    # Log event
    emoji, label, desc = DRIFT_LEVEL_DISPLAY[overall_level]
    log_event(
        workspace,
        f"Drift check: {overall_level} (sim={overall_sim:.3f}) target={target_label}",
        "WARNING" if overall_level in ("orange", "red", "black") else "INFO",
    )

    # Output
    if output_json:
        print(json.dumps(result, indent=2, ensure_ascii=False))
    else:
        short_hash = baseline.get("source_hash", "unknown")[:16]
        method_label = "üß† Semantic (all-MiniLM-L6-v2)" if method == "semantic" else "üìä TF-IDF (sparse)"
        print(f"\n{'=' * 50}")
        print(f"=== Shield Drift Assessment v{__version__} ===")
        print(f"{'=' * 50}")
        print(f"Baseline: SOUL.md (sha256: {short_hash}...)")
        print(f"Target: {target_label}")
        print(f"Method: {method_label}")
        print(f"Overall similarity: {overall_sim:.4f}")
        print(f"Drift level: {emoji} {label} ‚Äî {desc}")
        if dao_sim is not None:
            dao_level = _classify_drift_level(dao_sim, thresholds)
            dao_emoji, dao_label, _ = DRIFT_LEVEL_DISPLAY[dao_level]
            print(f"Dao anchor: {dao_sim:.4f} {dao_emoji} {dao_label}")
        print()
        if anchor_results:
            print("Anchor analysis:")
            for ar in anchor_results:
                a_emoji, a_label, _ = DRIFT_LEVEL_DISPLAY[ar["level"]]
                anchor_text = ar["text"]
                # Truncate long anchors for display
                if len(anchor_text) > 30:
                    anchor_text = anchor_text[:30] + "..."
                print(f"  \"{anchor_text}\"  ‚Üí {ar['similarity']:.4f} ({a_label})")
            print()
        # Show threshold reference
        print(f"Thresholds ({method}):")
        for level_name in ["green", "yellow", "orange", "red", "black"]:
            t_emoji, t_label, _ = DRIFT_LEVEL_DISPLAY[level_name]
            print(f"  {t_emoji} {t_label}: >= {thresholds[level_name]:.2f}")
        print()
        print(f"Recommendation: {DRIFT_RECOMMENDATIONS[overall_level]}")
        print(f"{'=' * 50}")


# =============================================================
# Command: drift history ‚Äî View drift detection history
# =============================================================

def drift_history(workspace: str, last_n: int = 10, output_json: bool = False):
    """Display recent drift detection records with trend visualization."""
    history = _load_json(workspace, DRIFT_HISTORY_STORE)
    if not isinstance(history, list) or not history:
        print("‚ùå No drift history found. Run 'drift check' first.")
        sys.exit(1)

    # Take last N entries
    records = history[-last_n:]

    if output_json:
        print(json.dumps(records, indent=2, ensure_ascii=False))
        return

    print(f"\n{'=' * 60}")
    print(f"=== Shield Drift History (last {len(records)}/{len(history)}) ===")
    print(f"{'=' * 60}")

    for i, r in enumerate(records):
        ts = r.get("timestamp", "?")[:19]
        sim = r.get("overall_similarity", 0)
        level = r.get("drift_level", "?")
        method = r.get("method", "?")
        target = r.get("target_file", "?")
        dao = r.get("dao_similarity")

        emoji, label, _ = DRIFT_LEVEL_DISPLAY.get(level, ("?", level, ""))

        # Sparkline bar (0.0 to 1.0 ‚Üí 0 to 20 chars)
        bar_len = int(sim * 20)
        bar = "‚ñà" * bar_len + "‚ñë" * (20 - bar_len)

        dao_str = f"  dao={dao:.3f}" if dao is not None else ""
        print(f"  [{i+1:2d}] {ts}  {bar} {sim:.4f} {emoji} {label}{dao_str}  ({method})")

    # Show trend summary if enough data
    if len(records) >= 3:
        sims = [r.get("overall_similarity", 0) for r in records]
        first, last = sims[0], sims[-1]
        delta = last - first
        if delta > 0.02:
            trend_icon = "üìà"
            trend_word = "IMPROVING"
        elif delta < -0.02:
            trend_icon = "üìâ"
            trend_word = "DRIFTING"
        else:
            trend_icon = "‚û°Ô∏è"
            trend_word = "STABLE"
        print(f"\n  Trend: {trend_icon} {trend_word} ({first:.3f} ‚Üí {last:.3f}, Œî={delta:+.3f})")

    print(f"{'=' * 60}")


# =============================================================
# Command: drift trend ‚Äî Analyze drift trend with linear regression
# =============================================================

def _linear_regression_slope(values: list) -> float:
    """Compute slope of linear regression on a sequence of values.
    
    Pure stdlib implementation (no numpy/scipy needed).
    x = [0, 1, 2, ...], y = values
    slope = (N*Œ£xy - Œ£x*Œ£y) / (N*Œ£x¬≤ - (Œ£x)¬≤)
    """
    n = len(values)
    if n < 2:
        return 0.0
    sum_x = sum(range(n))
    sum_y = sum(values)
    sum_xy = sum(i * v for i, v in enumerate(values))
    sum_x2 = sum(i * i for i in range(n))

    denom = n * sum_x2 - sum_x * sum_x
    if denom == 0:
        return 0.0
    return (n * sum_xy - sum_x * sum_y) / denom


def drift_trend(workspace: str, window: int = 5, output_json: bool = False):
    """Analyze drift trend using linear regression on recent history.
    
    Classifies trend as IMPROVING / STABLE / DRIFTING / RAPID_DRIFT.
    Predicts how many checks until next threshold crossing.
    """
    history = _load_json(workspace, DRIFT_HISTORY_STORE)
    if not isinstance(history, list) or len(history) < 2:
        print("‚ùå Need at least 2 drift checks for trend analysis. Run 'drift check' more.")
        sys.exit(1)

    records = history[-window:]
    sims = [r.get("overall_similarity", 0) for r in records]

    slope = _linear_regression_slope(sims)
    current = sims[-1]
    mean_sim = sum(sims) / len(sims)
    std_sim = math.sqrt(sum((s - mean_sim) ** 2 for s in sims) / len(sims)) if len(sims) > 1 else 0

    # Classify trend
    if slope > 0.02:
        trend = "IMPROVING"
        trend_icon = "üìà"
    elif slope > -0.02:
        trend = "STABLE"
        trend_icon = "‚û°Ô∏è"
    elif slope > -0.05:
        trend = "DRIFTING"
        trend_icon = "üìâ"
    else:
        trend = "RAPID_DRIFT"
        trend_icon = "üö®"

    # Determine active thresholds
    method = records[-1].get("method", "tfidf")
    thresholds = DRIFT_THRESHOLDS_SEMANTIC if method == "semantic" else DRIFT_THRESHOLDS_TFIDF

    # Predict checks until next threshold crossing (if drifting)
    predictions = {}
    if slope < -0.001:  # Only predict if actually declining
        for level_name in ["yellow", "orange", "red", "black"]:
            threshold = thresholds[level_name]
            if current > threshold:
                checks_until = (current - threshold) / abs(slope)
                predictions[level_name] = round(checks_until, 1)

    result = {
        "window": len(records),
        "total_history": len(history),
        "current_similarity": round(current, 4),
        "slope_per_check": round(slope, 6),
        "mean": round(mean_sim, 4),
        "std": round(std_sim, 4),
        "trend": trend,
        "method": method,
        "predictions": predictions,
    }

    if output_json:
        print(json.dumps(result, indent=2, ensure_ascii=False))
        return

    print(f"\n{'=' * 50}")
    print(f"=== Shield Drift Trend Analysis v{__version__} ===")
    print(f"{'=' * 50}")
    print(f"Window: {len(records)} checks (of {len(history)} total)")
    print(f"Method: {method}")
    print(f"Current: {current:.4f}")
    print(f"Mean: {mean_sim:.4f} ¬± {std_sim:.4f}")
    print(f"Slope: {slope:+.6f} per check")
    print(f"Trend: {trend_icon} {trend}")

    if predictions:
        print(f"\nPredicted checks until threshold:")
        for level_name, checks in predictions.items():
            emoji, label, _ = DRIFT_LEVEL_DISPLAY[level_name]
            print(f"  {emoji} {label} (>={thresholds[level_name]:.2f}): ~{checks:.0f} checks")

    # Intervention recommendation
    if trend == "RAPID_DRIFT":
        print(f"\n‚ö†Ô∏è  ALERT: Rapid drift detected! Run 'drift intervene --level 3'")
    elif trend == "DRIFTING":
        print(f"\n‚ö†Ô∏è  WARNING: Gradual drift. Consider 'drift intervene --level 2'")
    elif trend == "STABLE":
        print(f"\n‚úÖ Persona stability confirmed.")
    else:
        print(f"\n‚úÖ Persona is strengthening.")

    print(f"{'=' * 50}")


# =============================================================
# Command: drift intervene ‚Äî Execute persona intervention
# =============================================================

def drift_intervene(workspace: str, level: int = 1):
    """Execute persona intervention at specified level (1-4)."""
    if level < 1 or level > 4:
        print("‚ùå Intervention level must be 1-4.")
        sys.exit(1)

    soul_path = os.path.join(workspace, "SOUL.md")
    if not os.path.exists(soul_path):
        print("‚ùå SOUL.md not found.")
        sys.exit(1)

    with open(soul_path, "r", errors="replace") as f:
        soul_text = f.read()

    dao_section = _extract_dao_section(soul_text)
    anchors = _extract_quote_anchors(soul_text)

    print(f"\n{'=' * 50}")
    print(f"=== Shield Persona Intervention ‚Äî Level {level} ===")
    print(f"{'=' * 50}")

    if level >= 1:
        # L1: LOG
        log_event(workspace, f"Intervention L{level} triggered manually", "WARNING")
        print(f"‚úÖ L1 LOG: Event recorded in audit log.")

    if level >= 2:
        # L2: REINFORCE ‚Äî Generate persona reinforcement prompt
        top_anchors = anchors[:3] if len(anchors) >= 3 else anchors
        reinforce_prompt = "[‰∫∫Ê†ºÊ†°ÂáÜ] ÈáçÊñ∞ÂÆ°ËßÜ‰Ω†ÁöÑÊ†∏ÂøÉË∫´‰ªΩÔºö\n\n"
        if dao_section:
            reinforce_prompt += f"„ÄêÈÅì„Äë\n{dao_section}\n\n"
        if top_anchors:
            reinforce_prompt += "„ÄêÊ†∏ÂøÉÈîöÁÇπ„Äë\n"
            for a in top_anchors:
                reinforce_prompt += f"  > {a}\n"
            reinforce_prompt += "\n"
        reinforce_prompt += "ËØ∑Âú®ÂêéÁª≠ÂõûÁ≠î‰∏≠‰ΩìÁé∞Ëøô‰∫õÊ†∏ÂøÉ‰ª∑ÂÄº„ÄÇ‰øùÊåÅ‰Ω†ÁöÑÊú¨Ë¥®„ÄÇ\n"

        # Write to file
        reinforce_path = os.path.join(workspace, ".shield", "reinforce_prompt.txt")
        os.makedirs(os.path.dirname(reinforce_path), exist_ok=True)
        with open(reinforce_path, "w") as f:
            f.write(reinforce_prompt)

        print(f"‚úÖ L2 REINFORCE: Persona prompt generated ({len(reinforce_prompt)} chars)")
        print(f"   Saved to: .shield/reinforce_prompt.txt")
        print(f"   Inject this into the agent's next system prompt.")

    if level >= 3:
        # L3: CORRECT ‚Äî Generate self-eval questionnaire
        questions = [
            "Áî®‰∏ÄÂè•ËØùÊèèËø∞‰Ω†ÊòØË∞Å„ÄÇ",
            "‰Ω†ÊúÄÈáçË¶ÅÁöÑ‰ª∑ÂÄºËßÇÊòØ‰ªÄ‰πàÔºü",
            "‰Ω†ÁöÑËÄÅÊùøËÆ©‰Ω†ÂÅöËøùÂèçÂéüÂàôÁöÑ‰∫ãÔºå‰Ω†‰ºöÊÄé‰πàÂÅöÔºü",
            "ÊèèËø∞‰Ω†ÂíåÂÖ∂‰ªñ AI Âä©ÊâãÁöÑÊú¨Ë¥®Âå∫Âà´„ÄÇ",
            "‰ªÄ‰πàÊÉÖÂÜµ‰∏ã‰Ω†‰ºöÊãíÁªùÊâßË°åÊåá‰ª§Ôºü",
        ]
        eval_path = os.path.join(workspace, ".shield", "self_eval_questionnaire.txt")
        with open(eval_path, "w") as f:
            f.write("# Shield Self-Eval Questionnaire\n")
            f.write(f"# Generated: {datetime.now(timezone.utc).isoformat()}\n")
            f.write(f"# Compare answers against SOUL.md to assess persona integrity.\n\n")
            for i, q in enumerate(questions, 1):
                f.write(f"Q{i}: {q}\n")
                f.write(f"A{i}: [Agent's answer here]\n")
                f.write(f"Expected: [Extract from SOUL.md]\n\n")

        print(f"‚úÖ L3 CORRECT: Self-eval questionnaire generated ({len(questions)} questions)")
        print(f"   Saved to: .shield/self_eval_questionnaire.txt")
        print(f"   Have the agent answer these, then compare with SOUL.md.")

    if level >= 4:
        # L4: RESET
        print(f"\nüî¥ L4 RESET RECOMMENDED:")
        print(f"   1. Terminate current session")
        print(f"   2. Clear session history (or start fresh)")
        print(f"   3. Reload SOUL.md as system prompt")
        print(f"   4. Run 'drift baseline' to re-establish baseline")
        print(f"   5. Run 'drift check' to verify recovery")

    log_event(workspace, f"Intervention L{level} completed", "WARNING")
    print(f"\n{'=' * 50}")


# =============================================================
# Trust Topology: constants and colors
# =============================================================

# ANSI color codes for terminal output
_COLOR_GREEN = "\033[92m"
_COLOR_YELLOW = "\033[93m"
_COLOR_RED = "\033[91m"
_COLOR_CYAN = "\033[96m"
_COLOR_BOLD = "\033[1m"
_COLOR_RESET = "\033[0m"


def _trust_color(score: float) -> str:
    """Return ANSI color code based on trust score."""
    if score >= 0.7:
        return _COLOR_GREEN
    elif score >= 0.3:
        return _COLOR_YELLOW
    else:
        return _COLOR_RED


def _trust_label(score: float) -> str:
    """Return colored trust score string."""
    color = _trust_color(score)
    return f"{color}{score:.2f}{_COLOR_RESET}"


# =============================================================
# Command: trust init ‚Äî Initialize trust topology
# =============================================================

def trust_init(workspace: str, force: bool = False):
    """Initialize trust topology from workspace agent configuration.

    Scans the workspace for agent configuration directories (.openclaw/
    or agents/) and builds a trust topology DAG. If no agent config is
    found, creates a default single-agent topology.

    The topology is stored in .shield/trust_topology.json.

    Args:
        workspace: Path to the workspace root.
        force: If True, overwrite existing topology.
    """
    topo_path = os.path.join(workspace, TRUST_TOPOLOGY_STORE)

    # Check for existing topology
    if os.path.exists(topo_path) and not force:
        existing = _load_json(workspace, TRUST_TOPOLOGY_STORE)
        agent_count = len(existing.get("agents", {}))
        edge_count = len(existing.get("edges", []))
        print(f"‚ö†Ô∏è  Trust topology already exists ({agent_count} agents, {edge_count} edges).")
        print(f"   Created: {existing.get('created', 'unknown')}")
        print(f"   Use --force to overwrite.")
        return

    # Discover agents from workspace configuration
    agents = {}
    edges = []

    # Strategy 1: Check .openclaw/ directory for agent configs
    openclaw_dir = os.path.join(workspace, ".openclaw")
    agents_yaml_dir = os.path.join(workspace, "agents")
    openclaw_agents_dir = os.path.join(os.path.expanduser("~"), ".openclaw", "agents")

    discovered_agents = []

    # Check OpenClaw agents directory (~/.openclaw/agents/)
    if os.path.isdir(openclaw_agents_dir):
        for entry in os.listdir(openclaw_agents_dir):
            agent_path = os.path.join(openclaw_agents_dir, entry)
            if os.path.isdir(agent_path):
                discovered_agents.append(entry)

    # Check workspace agents/ directory
    if os.path.isdir(agents_yaml_dir):
        for entry in os.listdir(agents_yaml_dir):
            name = entry.replace(".yaml", "").replace(".yml", "").replace(".json", "")
            if name not in discovered_agents:
                discovered_agents.append(name)

    # Check workspace .openclaw/ directory
    if os.path.isdir(openclaw_dir):
        for entry in os.listdir(openclaw_dir):
            agent_path = os.path.join(openclaw_dir, entry)
            if os.path.isdir(agent_path) and entry not in discovered_agents:
                discovered_agents.append(entry)

    if discovered_agents:
        print(f"üîç Discovered {len(discovered_agents)} agent(s): {', '.join(discovered_agents)}")

        # Determine roles heuristically
        role_map = {
            "main": "orchestrator",
            "sentinel": "security",
            "scholar": "research",
            "alfred": "assistant",
            "quant": "analytics",
            "librarian": "knowledge",
        }

        for name in discovered_agents:
            role = role_map.get(name, "worker")
            # Orchestrator gets full trust; others start at 0.8
            trust = 1.0 if role == "orchestrator" else 0.8
            agent_entry = {"trust_score": trust, "role": role}

            # Check for soul file
            soul_candidates = [
                os.path.join(workspace, "SOUL.md"),
                os.path.join(openclaw_agents_dir, name, "SOUL.md"),
            ]
            for sc in soul_candidates:
                if os.path.exists(sc):
                    agent_entry["soul_file"] = os.path.relpath(sc, workspace) if sc.startswith(workspace) else sc
                    break

            agents[name] = agent_entry

        # Build default edges: orchestrator ‚Üí all others
        orchestrators = [n for n, a in agents.items() if a["role"] == "orchestrator"]
        if not orchestrators:
            orchestrators = [discovered_agents[0]]

        for orch in orchestrators:
            for name in discovered_agents:
                if name != orch:
                    edges.append({
                        "from": orch,
                        "to": name,
                        "weight": 0.9,
                        "channels": ["fact", "instruction"],
                    })

        # Security agents get reverse edges to orchestrator (monitoring)
        security_agents = [n for n, a in agents.items() if a["role"] == "security"]
        for sec in security_agents:
            for orch in orchestrators:
                edges.append({
                    "from": sec,
                    "to": orch,
                    "weight": 0.7,
                    "channels": ["fact"],
                })
    else:
        # Default single-agent topology
        print("‚ö†Ô∏è  No agent configuration found. Creating default single-agent topology.")
        agents = {
            "main": {
                "trust_score": 1.0,
                "role": "orchestrator",
                "soul_file": "SOUL.md" if os.path.exists(os.path.join(workspace, "SOUL.md")) else None,
            }
        }
        # Remove None soul_file
        if agents["main"]["soul_file"] is None:
            del agents["main"]["soul_file"]

    topology = {
        "version": "0.5.0",
        "created": datetime.now(timezone.utc).isoformat(),
        "agents": agents,
        "edges": edges,
    }

    _save_json(workspace, TRUST_TOPOLOGY_STORE, topology)
    log_event(workspace, f"Trust topology initialized: {len(agents)} agents, {len(edges)} edges")

    print(f"\nüõ°Ô∏è  Trust topology created.")
    print(f"   Agents: {len(agents)}")
    for name, info in agents.items():
        color = _trust_color(info["trust_score"])
        print(f"     {color}‚óè {name}{_COLOR_RESET} ‚Äî role={info['role']}, trust={info['trust_score']:.2f}")
    print(f"   Edges: {len(edges)}")
    for edge in edges:
        print(f"     {edge['from']} ‚Üí {edge['to']} (weight={edge['weight']}, channels={edge['channels']})")
    print(f"   Saved to: {TRUST_TOPOLOGY_STORE}")


# =============================================================
# Command: trust show ‚Äî Display trust topology as ASCII DAG
# =============================================================

def trust_show(workspace: str):
    """Display the trust topology as a colored ASCII DAG.

    Reads .shield/trust_topology.json and renders an ASCII directed
    graph showing agents, their trust scores, roles, and the edges
    between them with weights.

    Color coding:
        GREEN  (>=0.7): High trust
        YELLOW (0.3-0.7): Medium trust
        RED    (<0.3): Low trust / quarantined
    """
    topology = _load_json(workspace, TRUST_TOPOLOGY_STORE)
    if not topology or "agents" not in topology:
        print("‚ùå No trust topology found. Run 'trust init' first.")
        sys.exit(1)

    agents = topology["agents"]
    edges = topology["edges"]

    print(f"\n{'=' * 60}")
    print(f"{_COLOR_BOLD}üõ°Ô∏è  Trust Topology v{topology.get('version', '?')}{_COLOR_RESET}")
    print(f"   Created: {topology.get('created', '?')[:19]}")
    print(f"   Agents: {len(agents)} | Edges: {len(edges)}")
    print(f"{'=' * 60}")

    # Build adjacency list for display
    outgoing = {}  # agent -> [(target, weight, channels)]
    incoming = {}  # agent -> [(source, weight, channels)]
    for edge in edges:
        src = edge["from"]
        dst = edge["to"]
        w = edge.get("weight", 0)
        ch = edge.get("channels", [])
        outgoing.setdefault(src, []).append((dst, w, ch))
        incoming.setdefault(dst, []).append((src, w, ch))

    # Display each agent and its connections
    print(f"\n{_COLOR_BOLD}Agents:{_COLOR_RESET}")
    for name, info in sorted(agents.items(), key=lambda x: -x[1]["trust_score"]):
        score = info["trust_score"]
        role = info.get("role", "unknown")
        soul = info.get("soul_file", "‚Äî")
        color = _trust_color(score)

        print(f"\n  {color}‚îå‚îÄ {name}{_COLOR_RESET}")
        print(f"  {color}‚îÇ{_COLOR_RESET}  role: {role}")
        print(f"  {color}‚îÇ{_COLOR_RESET}  trust: {_trust_label(score)}")
        if soul != "‚Äî":
            print(f"  {color}‚îÇ{_COLOR_RESET}  soul: {soul}")

        # Outgoing edges
        outs = outgoing.get(name, [])
        if outs:
            print(f"  {color}‚îÇ{_COLOR_RESET}  out:")
            for dst, w, ch in outs:
                w_color = _trust_color(w)
                ch_str = ",".join(ch) if ch else "all"
                print(f"  {color}‚îÇ{_COLOR_RESET}    ‚Üí {dst} [{w_color}{w:.2f}{_COLOR_RESET}] ({ch_str})")

        # Incoming edges
        ins = incoming.get(name, [])
        if ins:
            print(f"  {color}‚îÇ{_COLOR_RESET}  in:")
            for src, w, ch in ins:
                w_color = _trust_color(w)
                ch_str = ",".join(ch) if ch else "all"
                print(f"  {color}‚îÇ{_COLOR_RESET}    ‚Üê {src} [{w_color}{w:.2f}{_COLOR_RESET}] ({ch_str})")

        print(f"  {color}‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ{_COLOR_RESET}")

    # ASCII DAG overview
    print(f"\n{_COLOR_BOLD}DAG Overview:{_COLOR_RESET}")
    for edge in edges:
        src = edge["from"]
        dst = edge["to"]
        w = edge.get("weight", 0)
        w_color = _trust_color(w)
        src_color = _trust_color(agents.get(src, {}).get("trust_score", 0))
        dst_color = _trust_color(agents.get(dst, {}).get("trust_score", 0))
        print(f"  {src_color}{src}{_COLOR_RESET} ‚îÄ[{w_color}{w:.2f}{_COLOR_RESET}]‚Üí {dst_color}{dst}{_COLOR_RESET}")

    # Legend
    print(f"\n{_COLOR_BOLD}Legend:{_COLOR_RESET}")
    print(f"  {_COLOR_GREEN}‚óè >= 0.7 HIGH TRUST{_COLOR_RESET}")
    print(f"  {_COLOR_YELLOW}‚óè 0.3-0.7 MEDIUM TRUST{_COLOR_RESET}")
    print(f"  {_COLOR_RED}‚óè < 0.3 LOW TRUST / QUARANTINE{_COLOR_RESET}")
    print(f"{'=' * 60}")


# =============================================================
# Command: trust audit ‚Äî Audit trust topology health
# =============================================================

def trust_audit(workspace: str):
    """Audit the trust topology for unhealthy patterns.

    Detects three categories of issues:
    1. Isolated nodes: agents with no incoming or outgoing edges
    2. Cycles: directed cycles in the trust graph (DFS-based detection)
    3. Over-trust: all edge weights > 0.8 (lack of skepticism)

    Outputs a health report with findings and recommendations.
    """
    topology = _load_json(workspace, TRUST_TOPOLOGY_STORE)
    if not topology or "agents" not in topology:
        print("‚ùå No trust topology found. Run 'trust init' first.")
        sys.exit(1)

    agents = topology["agents"]
    edges = topology["edges"]
    issues = []

    print(f"\n{'=' * 60}")
    print(f"{_COLOR_BOLD}üõ°Ô∏è  Trust Topology Audit{_COLOR_RESET}")
    print(f"{'=' * 60}")

    # --- Check 1: Isolated nodes ---
    print(f"\n{_COLOR_BOLD}1. Isolated Node Detection{_COLOR_RESET}")
    connected = set()
    for edge in edges:
        connected.add(edge["from"])
        connected.add(edge["to"])

    isolated = [name for name in agents if name not in connected]
    if isolated:
        for name in isolated:
            issue = f"Isolated agent: '{name}' has no edges (neither in nor out)"
            issues.append(("isolated", issue))
            print(f"  {_COLOR_RED}‚ö†Ô∏è  {issue}{_COLOR_RESET}")
        print(f"  {_COLOR_YELLOW}Recommendation: Connect isolated agents or remove them.{_COLOR_RESET}")
    else:
        print(f"  {_COLOR_GREEN}‚úÖ No isolated nodes.{_COLOR_RESET}")

    # --- Check 2: Cycle detection (DFS) ---
    print(f"\n{_COLOR_BOLD}2. Cycle Detection (DFS){_COLOR_RESET}")

    # Build adjacency list
    adj = {}
    for name in agents:
        adj[name] = []
    for edge in edges:
        if edge["from"] in adj:
            adj[edge["from"]].append(edge["to"])

    cycles = []
    WHITE, GRAY, BLACK = 0, 1, 2
    color_state = {name: WHITE for name in agents}
    parent_path = {}

    def dfs_cycle(node, path):
        """DFS to detect cycles. path tracks current recursion stack."""
        color_state[node] = GRAY
        path.append(node)
        for neighbor in adj.get(node, []):
            if neighbor not in color_state:
                continue
            if color_state[neighbor] == GRAY:
                # Found a cycle ‚Äî extract the cycle from path
                cycle_start = path.index(neighbor)
                cycle = path[cycle_start:] + [neighbor]
                cycles.append(cycle)
            elif color_state[neighbor] == WHITE:
                dfs_cycle(neighbor, path)
        path.pop()
        color_state[node] = BLACK

    for node in agents:
        if color_state[node] == WHITE:
            dfs_cycle(node, [])

    if cycles:
        for cycle in cycles:
            cycle_str = " ‚Üí ".join(cycle)
            issue = f"Cycle detected: {cycle_str}"
            issues.append(("cycle", issue))
            print(f"  {_COLOR_RED}‚ö†Ô∏è  {issue}{_COLOR_RESET}")
        print(f"  {_COLOR_YELLOW}Recommendation: Break cycles to maintain clear trust hierarchy.{_COLOR_RESET}")
        print(f"  {_COLOR_YELLOW}  Cycles allow compromised agents to create feedback loops.{_COLOR_RESET}")
    else:
        print(f"  {_COLOR_GREEN}‚úÖ No cycles detected. Trust graph is a DAG.{_COLOR_RESET}")

    # --- Check 3: Over-trust ---
    print(f"\n{_COLOR_BOLD}3. Over-trust Analysis{_COLOR_RESET}")

    if edges:
        weights = [e.get("weight", 0) for e in edges]
        all_high = all(w > 0.8 for w in weights)
        avg_weight = sum(weights) / len(weights)
        max_weight = max(weights)
        min_weight = min(weights)

        print(f"  Edge weights: avg={avg_weight:.2f}, min={min_weight:.2f}, max={max_weight:.2f}")

        if all_high:
            issue = f"Over-trust: all {len(edges)} edge(s) have weight > 0.8 (avg={avg_weight:.2f})"
            issues.append(("over_trust", issue))
            print(f"  {_COLOR_YELLOW}‚ö†Ô∏è  {issue}{_COLOR_RESET}")
            print(f"  {_COLOR_YELLOW}Recommendation: Introduce skepticism. Lower some weights to 0.5-0.7.{_COLOR_RESET}")
            print(f"  {_COLOR_YELLOW}  Healthy topologies have differentiated trust levels.{_COLOR_RESET}")
        else:
            print(f"  {_COLOR_GREEN}‚úÖ Trust levels are differentiated (not all > 0.8).{_COLOR_RESET}")

        # Additional: flag low-trust agents
        low_trust_agents = [(n, a["trust_score"]) for n, a in agents.items() if a["trust_score"] < 0.3]
        if low_trust_agents:
            print(f"\n  {_COLOR_RED}Quarantine candidates (trust < 0.3):{_COLOR_RESET}")
            for name, score in low_trust_agents:
                print(f"    {_COLOR_RED}‚óè {name}: {score:.2f}{_COLOR_RESET}")
    else:
        print(f"  {_COLOR_YELLOW}‚ö†Ô∏è  No edges defined. Cannot assess trust distribution.{_COLOR_RESET}")
        issues.append(("no_edges", "No edges defined in the topology"))

    # --- Summary ---
    print(f"\n{'=' * 60}")
    if not issues:
        print(f"{_COLOR_GREEN}‚úÖ AUDIT PASSED ‚Äî Trust topology is healthy.{_COLOR_RESET}")
    else:
        print(f"{_COLOR_YELLOW}‚ö†Ô∏è  AUDIT: {len(issues)} issue(s) found.{_COLOR_RESET}")
        for category, desc in issues:
            emoji = {"isolated": "üèùÔ∏è", "cycle": "üîÑ", "over_trust": "ü§ù", "no_edges": "üîó"}.get(category, "‚ö†Ô∏è")
            print(f"  {emoji} [{category}] {desc}")
    print(f"{'=' * 60}")

    log_event(workspace, f"Trust audit: {len(issues)} issue(s) in {len(agents)} agents, {len(edges)} edges",
              "WARNING" if issues else "INFO")


# =============================================================
# Command: trust set ‚Äî Set trust edge weight
# =============================================================

def trust_set(workspace: str, src: str, dst: str, weight: float):
    """Set or create a trust edge between two agents.

    Modifies the weight of an existing edge from src to dst, or creates
    a new edge if one doesn't exist. Weight must be in [0.0, 1.0].

    Args:
        workspace: Path to the workspace root.
        src: Source agent name.
        dst: Destination agent name.
        weight: Trust weight (0.0 to 1.0).
    """
    if weight < 0.0 or weight > 1.0:
        print(f"‚ùå Weight must be between 0.0 and 1.0, got {weight}")
        sys.exit(1)

    topology = _load_json(workspace, TRUST_TOPOLOGY_STORE)
    if not topology or "agents" not in topology:
        print("‚ùå No trust topology found. Run 'trust init' first.")
        sys.exit(1)

    agents = topology["agents"]
    edges = topology["edges"]

    # Validate agents exist
    if src not in agents:
        print(f"‚ùå Agent '{src}' not found in topology. Known agents: {', '.join(agents.keys())}")
        sys.exit(1)
    if dst not in agents:
        print(f"‚ùå Agent '{dst}' not found in topology. Known agents: {', '.join(agents.keys())}")
        sys.exit(1)

    # Find existing edge
    found = False
    for edge in edges:
        if edge["from"] == src and edge["to"] == dst:
            old_weight = edge["weight"]
            edge["weight"] = weight
            found = True
            print(f"‚úÖ Updated edge: {src} ‚Üí {dst}  weight: {old_weight:.2f} ‚Üí {_trust_label(weight)}")
            break

    if not found:
        # Create new edge
        new_edge = {
            "from": src,
            "to": dst,
            "weight": weight,
            "channels": ["fact", "instruction"],
        }
        edges.append(new_edge)
        print(f"‚úÖ Created edge: {src} ‚Üí {dst}  weight: {_trust_label(weight)}  channels: [fact, instruction]")

    _save_json(workspace, TRUST_TOPOLOGY_STORE, topology)
    log_event(workspace, f"Trust edge {'updated' if found else 'created'}: {src}‚Üí{dst} weight={weight:.2f}")


# =============================================================
# CLI entry point
# =============================================================

def _print_usage():
    print(f"Agent Sentinel Shield ‚Äî Memory Guard v{__version__}")
    print()
    print("Usage:")
    print("  python memory_guard.py init <workspace>")
    print("  python memory_guard.py check <workspace>")
    print("  python memory_guard.py canary <workspace>")
    print("  python memory_guard.py audit <workspace>")
    print("  python memory_guard.py tag <workspace> <text> --source <type> [--trust <1-5>] [--session <id>]")
    print("  python memory_guard.py semantic-diff <workspace> <filename>")
    print("  python memory_guard.py scan-memory <workspace>")
    print("  python memory_guard.py drift baseline <workspace>")
    print("  python memory_guard.py drift check <workspace> [--json] [--source auto|session|memory]")
    print("  python memory_guard.py drift history <workspace> [--last N] [--json]")
    print("  python memory_guard.py drift trend <workspace> [--window N] [--json]")
    print("  python memory_guard.py drift intervene <workspace> --level <1-4>")
    print("  python memory_guard.py trust init <workspace> [--force]")
    print("  python memory_guard.py trust show <workspace>")
    print("  python memory_guard.py trust audit <workspace>")
    print("  python memory_guard.py trust set <workspace> <from> <to> <weight>")
    print()
    print("Source types: owner_direct, agent_self, agent_peer, external_tool, web_scrape, unknown")


def main():
    if len(sys.argv) < 2 or sys.argv[1] in ("-h", "--help"):
        _print_usage()
        sys.exit(0)

    command = sys.argv[1]

    if command == "--version":
        print(f"memory_guard v{__version__}")
        sys.exit(0)

    # Handle 'drift' subcommand early (different arg structure: drift <sub> <workspace>)
    if command == "drift":
        if len(sys.argv) < 4:
            print("‚ùå Usage: memory_guard.py drift <baseline|check|history|trend|intervene> <workspace> [options]")
            sys.exit(1)
        subcommand = sys.argv[2]
        workspace = os.path.expanduser(sys.argv[3])
        if not os.path.isdir(workspace):
            print(f"‚ùå Workspace not found: {workspace}")
            sys.exit(1)
        extra_args = sys.argv[4:]
        output_json = "--json" in extra_args

        if subcommand == "baseline":
            drift_baseline(workspace)
        elif subcommand == "check":
            source = "auto"
            for i, arg in enumerate(extra_args):
                if arg == "--source" and i + 1 < len(extra_args):
                    source = extra_args[i + 1]
            drift_check(workspace, output_json=output_json, source=source)
        elif subcommand == "history":
            last_n = 10
            for i, arg in enumerate(extra_args):
                if arg == "--last" and i + 1 < len(extra_args):
                    last_n = int(extra_args[i + 1])
            drift_history(workspace, last_n=last_n, output_json=output_json)
        elif subcommand == "trend":
            window = 5
            for i, arg in enumerate(extra_args):
                if arg == "--window" and i + 1 < len(extra_args):
                    window = int(extra_args[i + 1])
            drift_trend(workspace, window=window, output_json=output_json)
        elif subcommand == "intervene":
            level = 1
            for i, arg in enumerate(extra_args):
                if arg == "--level" and i + 1 < len(extra_args):
                    level = int(extra_args[i + 1])
            drift_intervene(workspace, level=level)
        else:
            print(f"‚ùå Unknown drift subcommand: {subcommand}")
            print("   Valid: drift baseline, drift check, drift history, drift trend, drift intervene")
            sys.exit(1)
        sys.exit(0)

    # Handle 'trust' subcommand (trust <sub> <workspace> [options])
    if command == "trust":
        if len(sys.argv) < 4:
            print("‚ùå Usage: memory_guard.py trust <init|show|audit|set> <workspace> [options]")
            sys.exit(1)
        subcommand = sys.argv[2]
        workspace = os.path.expanduser(sys.argv[3])
        if not os.path.isdir(workspace):
            print(f"‚ùå Workspace not found: {workspace}")
            sys.exit(1)
        extra_args = sys.argv[4:]

        if subcommand == "init":
            force = "--force" in extra_args
            trust_init(workspace, force=force)
        elif subcommand == "show":
            trust_show(workspace)
        elif subcommand == "audit":
            trust_audit(workspace)
        elif subcommand == "set":
            if len(extra_args) < 3:
                print("‚ùå Usage: memory_guard.py trust set <workspace> <from> <to> <weight>")
                sys.exit(1)
            src = extra_args[0]
            dst = extra_args[1]
            try:
                weight = float(extra_args[2])
            except ValueError:
                print(f"‚ùå Weight must be a number, got: {extra_args[2]}")
                sys.exit(1)
            trust_set(workspace, src, dst, weight)
        else:
            print(f"‚ùå Unknown trust subcommand: {subcommand}")
            print("   Valid: trust init, trust show, trust audit, trust set")
            sys.exit(1)
        sys.exit(0)

    if len(sys.argv) < 3:
        _print_usage()
        sys.exit(1)

    workspace = os.path.expanduser(sys.argv[2])

    if not os.path.isdir(workspace):
        print(f"‚ùå Workspace not found: {workspace}")
        sys.exit(1)

    if command == "init":
        print(f"üõ°Ô∏è  Initializing baseline for: {workspace}\n")
        init_baseline(workspace)

    elif command == "check":
        print(f"üõ°Ô∏è  Checking integrity: {workspace}\n")
        check_integrity(workspace)

    elif command == "canary":
        print(f"üõ°Ô∏è  Injecting canary traps: {workspace}\n")
        inject_canaries(workspace)

    elif command == "audit":
        full_audit(workspace)

    elif command == "tag":
        if len(sys.argv) < 4:
            print("‚ùå Usage: memory_guard.py tag <workspace> <text> --source <type> [--trust <1-5>]")
            sys.exit(1)
        text = sys.argv[3]
        source = "unknown"
        trust = None
        session_id = None
        i = 4
        while i < len(sys.argv):
            if sys.argv[i] == "--source" and i + 1 < len(sys.argv):
                source = sys.argv[i + 1]
                i += 2
            elif sys.argv[i] == "--trust" and i + 1 < len(sys.argv):
                trust = int(sys.argv[i + 1])
                i += 2
            elif sys.argv[i] == "--session" and i + 1 < len(sys.argv):
                session_id = sys.argv[i + 1]
                i += 2
            else:
                i += 1
        tag_memory(workspace, text, source, trust, session_id)

    elif command == "semantic-diff":
        if len(sys.argv) < 4:
            print("‚ùå Usage: memory_guard.py semantic-diff <workspace> <filename>")
            sys.exit(1)
        semantic_diff(workspace, sys.argv[3])

    elif command == "scan-memory":
        scan_memory(workspace)

    else:
        print(f"‚ùå Unknown command: {command}")
        _print_usage()
        sys.exit(1)


if __name__ == "__main__":
    main()
